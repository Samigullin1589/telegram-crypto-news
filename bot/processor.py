# bot/processor.py
import asyncio
import aiohttp
import feedparser
import time
from urllib.parse import urlparse, urlunparse
from . import config
from .database import DatabaseManager
from .ai_handler import AIHandler
from .telegram_poster import TelegramPoster
from .content_parser import ContentParser

class NewsProcessor:
    def __init__(self):
        self.db = DatabaseManager()
        self.ai = AIHandler()
        self.poster = TelegramPoster()
        self.parser = ContentParser()
        self.posted_urls_cache = set()
    
    def _normalize_url(self, url):
        """–û—á–∏—â–∞–µ—Ç URL –æ—Ç –≤—Å–µ—Ö query-–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (UTM –∏ —Ç.–¥.)."""
        parts = urlparse(url)
        return urlunparse((parts.scheme, parts.netloc, parts.path, '', '', ''))
    
    async def _fetch_and_parse_feed(self, category, url, session):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –ø–∞—Ä—Å–∏—Ç –æ–¥–Ω—É RSS-–ª–µ–Ω—Ç—É."""
        try:
            print(f"üì° [FETCH] –ó–∞–ø—Ä–∞—à–∏–≤–∞—é: {category}")
            async with session.get(url, timeout=20) as response:
                if response.status != 200:
                    print(f"üï∏Ô∏è [WARN] –ò—Å—Ç–æ—á–Ω–∏–∫ '{category}' –≤–µ—Ä–Ω—É–ª —Å—Ç–∞—Ç—É—Å {response.status}")
                    return []
                feed_bytes = await response.read()
            
            loop = asyncio.get_event_loop()
            feed = await loop.run_in_executor(None, feedparser.parse, feed_bytes)
            
            if feed.bozo:
                print(f"üï∏Ô∏è [WARN] RSS-–ª–µ–Ω—Ç–∞ –¥–ª—è '{category}' –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π. –ü—Ä–∏—á–∏–Ω–∞: {feed.bozo_exception}")
            
            new_entries = []
            for entry in feed.entries:
                original_link = entry.get('link')
                if not original_link: continue
                
                normalized_link = self._normalize_url(original_link)
                if normalized_link not in self.posted_urls_cache:
                    new_entries.append((entry, category))
            
            print(f"üì∞ [FETCH] –ü—Ä–æ–≤–µ—Ä–µ–Ω–æ: {category}. –ù–∞–π–¥–µ–Ω–æ –Ω–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π: {len(new_entries)}")
            return new_entries
        except Exception as e:
            print(f"‚ùå [CRITICAL] –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å RSS-–ª–µ–Ω—Ç—É {category}: {e}")
            return []

    async def _fetch_feed_entries(self, session):
        """–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –∑–∞–ø—É—Å–∫–∞–µ—Ç –∑–∞–≥—Ä—É–∑–∫—É –≤—Å–µ—Ö RSS-–ª–µ–Ω—Ç."""
        tasks = [self._fetch_and_parse_feed(cat, url, session) for cat, url in config.RSS_FEEDS.items()]
        results = await asyncio.gather(*tasks)
        return [entry for feed_result in results for entry in feed_result]

    async def run(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª —Ä–∞–±–æ—Ç—ã –±–æ—Ç–∞ —Å –∫–æ—Ä–æ—Ç–∫–æ–∂–∏–≤—É—â–∏–º–∏ —Å–µ—Å—Å–∏—è–º–∏."""
        self.posted_urls_cache = self.db.get_all_links()
        
        # –§–ª–∞–≥ –¥–ª—è –æ–¥–Ω–æ–∫—Ä–∞—Ç–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ª–æ–≥–∏–∫–∏ –ø–µ—Ä–≤–æ–≥–æ –∑–∞–ø—É—Å–∫–∞
        is_first_run = not self.posted_urls_cache
        
        print(f"‚úÖ [START] –ë–æ—Ç –≤ —Ä–∞–±–æ—á–µ–º —Ä–µ–∂–∏–º–µ. –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.posted_urls_cache)} —Å—Å—ã–ª–æ–∫.")

        while True:
            all_new_entries = []
            
            # --- –ì–õ–ê–í–ù–û–ï –ò–ó–ú–ï–ù–ï–ù–ò–ï: –°–µ—Å—Å–∏—è —Å–æ–∑–¥–∞—ë—Ç—Å—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ü–∏–∫–ª–∞ ---
            async with aiohttp.ClientSession(headers=config.COMMON_HEADERS) as session:
                if is_first_run:
                    print("üî• [FIRST RUN] –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –ø—É—Å—Ç–∞. –ó–∞–ø–æ–ª–Ω—è—é –µ–µ —Ç–µ–∫—É—â–∏–º–∏ —Å—Ç–∞—Ç—å—è–º–∏...")
                    # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç—å–∏ –¥–ª—è baseline, –Ω–æ –Ω–µ –ø—É–±–ª–∏–∫—É–µ–º –∏—Ö —Å—Ä–∞–∑—É
                    baseline_entries = await self._fetch_feed_entries(session)
                    baseline_links = {self._normalize_url(entry[0].get('link')) for entry in baseline_entries if entry[0].get('link')}
                    if baseline_links:
                        self.db.save_links_bulk(baseline_links)
                        self.posted_urls_cache.update(baseline_links)
                        print(f"‚úÖ [BASELINE] –ë–∞–∑–æ–≤–∞—è –ª–∏–Ω–∏—è —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞. –í –±–∞–∑—É –¥–æ–±–∞–≤–ª–µ–Ω–æ {len(baseline_links)} —Å—Ç–∞—Ç–µ–π.")
                    is_first_run = False # –í—ã–∫–ª—é—á–∞–µ–º —Ñ–ª–∞–≥
                
                # –û—Å–Ω–æ–≤–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
                print(f"\n--- [CYCLE] –ù–æ–≤–∞—è –∏—Ç–µ—Ä–∞—Ü–∏—è –ø—Ä–æ–≤–µ—Ä–∫–∏: {time.ctime()} ---")
                all_new_entries = await self._fetch_feed_entries(session)

                if all_new_entries:
                    sorted_entries = sorted(all_new_entries, key=lambda x: x[0].get('published_parsed', time.gmtime()))
                    print(f"üî• [QUEUE] –ù–∞–π–¥–µ–Ω–æ {len(sorted_entries)} –Ω–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π. –ù–∞—á–∏–Ω–∞—é –ø—É–±–ª–∏–∫–∞—Ü–∏—é –ø–æ –æ—á–µ—Ä–µ–¥–∏.")
                    
                    for entry, category in sorted_entries:
                        original_link = entry.get('link')
                        title = entry.get('title', '–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞')
                        if not original_link: continue

                        normalized_link = self._normalize_url(original_link)
                        if normalized_link in self.posted_urls_cache: continue

                        print(f"\nüîç [PROCESS] –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é: {title} ({category})")
                        content = await self.parser.get_article_content(original_link, entry, session)
                        
                        final_link = content.get('final_url', original_link)
                        formatted_post = await self.ai.get_summary(title, content['text'], category)

                        if formatted_post:
                            success = await self.poster.post(formatted_post, final_link, content['image_url'])
                            if success:
                                self.db.save_link(normalized_link)
                                self.posted_urls_cache.add(normalized_link)
                                print(f"üïí [PAUSE] –ü—É–±–ª–∏–∫–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–∞. –°–ª–µ–¥—É—é—â–∞—è —á–µ—Ä–µ–∑ {config.POST_DELAY_SECONDS / 60:.0f} –º–∏–Ω—É—Ç.")
                                await asyncio.sleep(config.POST_DELAY_SECONDS)
                        else:
                            print("‚ùå [SKIP] –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Å–∞–º–º–∞—Ä–∏. –ü—Ä–æ–ø—É—Å–∫–∞—é –Ω–æ–≤–æ—Å—Ç—å.")
                            await asyncio.sleep(5)
                else:
                    print("üëç [INFO] –ù–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")

            # –°–µ—Å—Å–∏—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–∫—Ä—ã–≤–∞–µ—Ç—Å—è –∑–¥–µ—Å—å –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ —Ü–∏–∫–ª–∞
            
            print(f"--- [PAUSE] –°–ª–µ–¥—É—é—â–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ {config.IDLE_DELAY_SECONDS / 60:.0f} –º–∏–Ω—É—Ç. ---")
            await asyncio.sleep(config.IDLE_DELAY_SECONDS)