# bot/processor.py
import asyncio
import aiohttp
import feedparser
import time
from urllib.parse import urlparse, urlunparse
from . import config
from .database import DatabaseManager
from .ai_handler import AIHandler
from .telegram_poster import TelegramPoster
from .content_parser import ContentParser

class NewsProcessor:
    def __init__(self):
        self.db = DatabaseManager()
        self.ai = AIHandler()
        self.poster = TelegramPoster()
        self.parser = ContentParser()
        self.posted_urls_cache = set()
    
    def _normalize_url(self, url):
        parts = urlparse(url)
        return urlunparse((parts.scheme, parts.netloc, parts.path, '', '', ''))
    
    async def _fetch_and_parse_feed(self, category, url, session):
        try:
            print(f"üì° [FETCH] –ó–∞–ø—Ä–∞—à–∏–≤–∞—é: {category}")
            async with session.get(url, timeout=20) as response:
                if response.status != 200:
                    print(f"üï∏Ô∏è [WARN] –ò—Å—Ç–æ—á–Ω–∏–∫ '{category}' –≤–µ—Ä–Ω—É–ª —Å—Ç–∞—Ç—É—Å {response.status}")
                    return []
                feed_bytes = await response.read()
            loop = asyncio.get_event_loop()
            feed = await loop.run_in_executor(None, feedparser.parse, feed_bytes)
            if feed.bozo:
                print(f"üï∏Ô∏è [WARN] RSS-–ª–µ–Ω—Ç–∞ –¥–ª—è '{category}' –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π. –ü—Ä–∏—á–∏–Ω–∞: {feed.bozo_exception}")
            new_entries = []
            for entry in feed.entries:
                original_link = entry.get('link')
                if not original_link: continue
                normalized_link = self._normalize_url(original_link)
                if normalized_link not in self.posted_urls_cache:
                    new_entries.append((entry, category))
            print(f"üì∞ [FETCH] –ü—Ä–æ–≤–µ—Ä–µ–Ω–æ: {category}. –ù–∞–π–¥–µ–Ω–æ –Ω–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π: {len(new_entries)}")
            return new_entries
        except Exception as e:
            print(f"‚ùå [CRITICAL] –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å RSS-–ª–µ–Ω—Ç—É {category}: {e}")
            return []

    async def _fetch_feed_entries(self, session):
        tasks = [self._fetch_and_parse_feed(cat, url, session) for cat, url in config.RSS_FEEDS.items()]
        results = await asyncio.gather(*tasks)
        return [entry for feed_result in results for entry in feed_result]

    async def run(self):
        self.posted_urls_cache = self.db.get_all_links()
        all_new_entries = []
        
        async with aiohttp.ClientSession() as session:
            if not self.posted_urls_cache:
                print("üî• [FIRST RUN] –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –ø—É—Å—Ç–∞. –ó–∞–ø–æ–ª–Ω—è—é –µ–µ —Ç–µ–∫—É—â–∏–º–∏ —Å—Ç–∞—Ç—å—è–º–∏...")
                all_new_entries = await self._fetch_feed_entries(session)
                baseline_links = {self._normalize_url(entry[0].get('link')) for entry in all_new_entries if entry[0].get('link')}
                if baseline_links:
                    self.db.save_links_bulk(baseline_links)
                    self.posted_urls_cache.update(baseline_links)
                    print(f"‚úÖ [BASELINE] –ë–∞–∑–æ–≤–∞—è –ª–∏–Ω–∏—è —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞. –í –±–∞–∑—É –¥–æ–±–∞–≤–ª–µ–Ω–æ {len(baseline_links)} —Å—Ç–∞—Ç–µ–π.")
            
            print(f"‚úÖ [START] –ë–æ—Ç –≤ —Ä–∞–±–æ—á–µ–º —Ä–µ–∂–∏–º–µ. –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.posted_urls_cache)} —Å—Å—ã–ª–æ–∫.")

            while True:
                if not all_new_entries:
                    print(f"\n--- [CYCLE] –ù–æ–≤–∞—è –∏—Ç–µ—Ä–∞—Ü–∏—è –ø—Ä–æ–≤–µ—Ä–∫–∏: {time.ctime()} ---")
                    all_new_entries = await self._fetch_feed_entries(session)

                if all_new_entries:
                    sorted_entries = sorted(all_new_entries, key=lambda x: x[0].get('published_parsed', time.gmtime()))
                    print(f"üî• [QUEUE] –ù–∞–π–¥–µ–Ω–æ {len(sorted_entries)} –Ω–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π. –ù–∞—á–∏–Ω–∞—é –ø—É–±–ª–∏–∫–∞—Ü–∏—é –ø–æ –æ—á–µ—Ä–µ–¥–∏.")
                    
                    for entry, category in sorted_entries:
                        original_link = entry.get('link')
                        title = entry.get('title', '–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞')
                        if not original_link: continue
                        normalized_link = self._normalize_url(original_link)
                        if normalized_link in self.posted_urls_cache: continue

                        print(f"\nüîç [PROCESS] –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é: {title} ({category})")
                        content = await self.parser.get_article_content(original_link, entry, session)
                        
                        final_link = content.get('final_url', original_link)
                        formatted_post = await self.ai.get_summary(title, content['text'], category)

                        if formatted_post:
                            success = await self.poster.post(formatted_post, final_link, content['image_url'])
                            if success:
                                self.db.save_link(normalized_link)
                                self.posted_urls_cache.add(normalized_link)
                                print(f"üïí [PAUSE] –ü—É–±–ª–∏–∫–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–∞. –°–ª–µ–¥—É—é—â–∞—è —á–µ—Ä–µ–∑ {config.POST_DELAY_SECONDS / 60:.0f} –º–∏–Ω—É—Ç.")
                                await asyncio.sleep(config.POST_DELAY_SECONDS)
                        else:
                            print("‚ùå [SKIP] –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Å–∞–º–º–∞—Ä–∏. –ü—Ä–æ–ø—É—Å–∫–∞—é –Ω–æ–≤–æ—Å—Ç—å.")
                            await asyncio.sleep(5)
                else:
                    print("üëç [INFO] –ù–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")

                all_new_entries = []
                print(f"--- [PAUSE] –°–ª–µ–¥—É—é—â–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ {config.IDLE_DELAY_SECONDS / 60:.0f} –º–∏–Ω—É—Ç. ---")
                await asyncio.sleep(config.IDLE_DELAY_SECONDS)